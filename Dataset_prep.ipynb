{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from textacy import text_stats, make_spacy_doc\n",
    "from datasets import Dataset, load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "kept_prompts = pd.read_csv(\"UF10k_mixedbread_topics.csv\") #import the prompts we decided to keep after topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_human = pd.DataFrame(load_dataset(\"argilla/ultrafeedback-binarized-preferences-cleaned\")[\"train\"]).sample(n=15000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_human_short = data_human.sample(n=4670).reset_index(drop=True) # Get sample random sample of the same size as our metric data\n",
    "data_human_long = data_human.sample(n=9340).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Data for fine tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Helper stuff:\n",
    "# data_to_drop = [9849, 1941, 4635, 7232, 7789] #first 3 diff langs, last 2 had NAs in them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_human_short['chosen'] = data_human_short['chosen'].apply(lambda x: x[1]['content'])\n",
    "data_human_short['rejected'] = data_human_short['rejected'].apply(lambda x: x[1]['content'])\n",
    "data_human_short = data_human_short.loc[:, ['prompt', 'chosen', 'chosen-rating', 'rejected', 'rejected-rating']]\n",
    "# data_human = data_human.drop(data_to_drop).reset_index(drop=True)\n",
    "data_human_short['differences'] = data_human_short.loc[:, 'chosen-rating'] - data_human_short.loc[:, 'rejected-rating']\n",
    "\n",
    "data_human_short.to_csv('human_short.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_human_long['chosen'] = data_human_long['chosen'].apply(lambda x: x[1]['content'])\n",
    "data_human_long['rejected'] = data_human_long['rejected'].apply(lambda x: x[1]['content'])\n",
    "data_human_long = data_human_long.loc[:, ['prompt', 'chosen', 'chosen-rating', 'rejected', 'rejected-rating']]\n",
    "# data_human = data_human.drop(data_to_drop).reset_index(drop=True)\n",
    "data_human_long['differences'] = data_human_long.loc[:, 'chosen-rating'] - data_human_long.loc[:, 'rejected-rating']\n",
    "\n",
    "data_human_long.to_csv('human_double.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_human['chosen'] = data_human['chosen'].apply(lambda x: x[1]['content'])\n",
    "data_human['rejected'] = data_human['rejected'].apply(lambda x: x[1]['content'])\n",
    "data_human = data_human.loc[:, ['prompt', 'chosen', 'chosen-rating', 'rejected', 'rejected-rating']]\n",
    "# data_human = data_human.drop(data_to_drop).reset_index(drop=True)\n",
    "data_human['differences'] = data_human.loc[:, 'chosen-rating'] - data_human.loc[:, 'rejected-rating']\n",
    "\n",
    "data_human_diffsort = data_human.sort_values(by='differences', ascending=False)[:4670].reset_index(drop=True)\n",
    "data_human_maxsort = data_human.sort_values(by='chosen-rating', ascending=False)[:4670].reset_index(drop=True)\n",
    "\n",
    "data_human_diffsort.to_csv('human_diffsort.csv')\n",
    "data_human_diffsort.to_csv('human_maxsort.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric Data for fine tuning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_linear = pd.read_csv('linear_metric_df.csv')\n",
    "data_linear = data_linear.loc[:, ['prompt', 'chosen', 'rejected', 'accepted_linear_metric', 'rejected_linear_metric', 'diff_linear_metric']]\n",
    "# np.where(data_linear.isna())\n",
    "# data_linear.sort_values(by = 'accepted_linear_metric', ascending=False) # Judging by this, we have some foregin language issues that is distorting the score so we will\n",
    "# # take these rows out.\n",
    "# data_linear = data_linear.drop([9849, 1941, 4635, 7234, 7791]).reset_index(drop=True) #this goes into data_to_drop because of above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we need to swap around the parts where our score alledges the rejected is better than the chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bad_idx_finder(DataFrame):\n",
    "    return np.where(DataFrame.iloc[:, 3] < DataFrame.iloc[:, 4])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_switcher(badidxs: list, DataFrame): \n",
    "    storage = {'metric_accept': [], 'metric_accept_score': [], 'metric_reject': [], 'metric_reject_score': []}\n",
    "\n",
    "    #populate the storage dict\n",
    "    for idx in badidxs:\n",
    "        storage['metric_accept'].append(DataFrame.iloc[idx, 2]) \n",
    "        storage['metric_accept_score'].append(DataFrame.iloc[idx, 4])\n",
    "\n",
    "        storage['metric_reject'].append(DataFrame.iloc[idx, 1])\n",
    "        storage['metric_reject_score'].append(DataFrame.iloc[idx, 3])\n",
    "\n",
    "    mask = DataFrame.index.isin(badidxs)\n",
    "\n",
    "    DataFrame.loc[mask, DataFrame.columns[1]] = storage['metric_accept']\n",
    "    DataFrame.loc[mask, DataFrame.columns[3]] = storage['metric_accept_score']\n",
    "\n",
    "    DataFrame.loc[mask, DataFrame.columns[2]] = storage['metric_reject']\n",
    "    DataFrame.loc[mask, DataFrame.columns[4]] = storage['metric_reject_score']\n",
    "\n",
    "    DataFrame['absolute_difference'] = abs(DataFrame.iloc[:, -1])\n",
    "\n",
    "    return DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storage = {'linear_accept': [], 'linear_accept_score': [], 'linear_reject': [], 'linear_reject_score': []} #initiate dict to store values of indexes to switch\n",
    "\n",
    "# for i in bad_lin_idx: #loop through to populate the dict\n",
    "#     storage['linear_accept'].append(data_linear.iloc[i]['rejected']) \n",
    "#     storage['linear_reject'].append(data_linear.iloc[i]['chosen'])\n",
    "#     storage['linear_accept_score'].append(data_linear.iloc[i]['rejected_linear_metric'])\n",
    "#     storage['linear_reject_score'].append(data_linear.iloc[i]['accepted_linear_metric'])\n",
    "\n",
    "# mask = data_linear.index.isin(bad_lin_idx) #make a index map for locing \n",
    "\n",
    "# # do the switching\n",
    "# data_linear.loc[mask, 'chosen'] = storage['linear_accept']\n",
    "# data_linear.loc[mask, 'rejected'] = storage['linear_reject']\n",
    "# data_linear.loc[mask, 'accepted_linear_metric'] = storage['linear_accept_score']\n",
    "# data_linear.loc[mask, 'rejected_linear_metric'] = storage['linear_reject_score']\n",
    "\n",
    "\n",
    "# data_linear['absolute_difference'] = abs(data_linear['diff_linear_metric']) #absolute difference will reflect the updated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_idxs = bad_idx_finder(data_linear)\n",
    "data_linear = data_switcher(linear_idxs, data_linear)\n",
    "data_linear_diffpreped = data_linear.sort_values(by='absolute_difference', ascending=False).reset_index(drop=True) # this dataset maximizes the difference in score between Chosen and Rejected\n",
    "data_linear_maxpreped = data_linear.sort_values(by='accepted_linear_metric', ascending=False).reset_index(drop=True)# this dataset maximizes the raw Chosen score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reversing the bunka score order as control\n",
    "\n",
    "data_linear_diffpreped_reversed = pd.DataFrame({})\n",
    "data_linear_diffpreped_reversed['prompt'] = data_linear_diffpreped['prompt']\n",
    "data_linear_diffpreped_reversed['chosen'] = data_linear_diffpreped['rejected']\n",
    "data_linear_diffpreped_reversed['rejected'] = data_linear_diffpreped['chosen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_linear_diffpreped.to_csv('diff_prepped_linear_metric.csv')\n",
    "data_linear_maxpreped.to_csv('max_preped_linear_metric.csv')\n",
    "data_linear_diffpreped_reversed.to_csv('diff_prepped_linear_metric_REVERSED.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pca = pd.read_csv('pca_metric_df.csv')\n",
    "data_pca = data_pca.loc[:, ['prompt', 'chosen', 'rejected', 'accepted_pca1', 'rejected_pca1', 'diff_pca1']]\n",
    "data_pca = data_pca.drop(data_to_drop).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_idxs = bad_idx_finder(data_pca)\n",
    "data_pca = data_switcher(pca_idxs, data_pca)\n",
    "data_pca_diff_prepped = data_pca.sort_values(by='absolute_difference', ascending=False)\n",
    "data_pca_max_prepped = data_pca.sort_values(by='accepted_pca1', ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_weighted_pca = pd.read_csv('weighted_pca.csv')\n",
    "data_weighted_pca = data_weighted_pca.loc[:, ['prompt', 'chosen', 'rejected', 'accepted_wpca', 'rejected_wpca', 'diff_wpca']]\n",
    "data_weighted_pca = data_weighted_pca.drop(data_to_drop).reset_index(drop=True) #proabbly dont need the data to drop anymore because its taken care of on the backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wpca_idxs = bad_idx_finder(data_weighted_pca)\n",
    "data_weighted_pca = data_switcher(wpca_idxs, data_weighted_pca)\n",
    "data_wpca_diff_prepped = data_weighted_pca.sort_values(by='absolute_difference', ascending=False)\n",
    "data_wpca_max_prepped = data_weighted_pca.sort_values(by='accepted_wpca', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
